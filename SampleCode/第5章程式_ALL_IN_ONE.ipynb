{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g7uxbK848ov"
   },
   "source": [
    "# 程式 5.1: 介紹多程序處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixk1Kc3b4mrt",
    "outputId": "be81e8dd-1cdd-447c-b0bb-347c5007d4f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "# from multiprocess import queues\n",
    "import numpy as np\n",
    "def square(x): #將陣列輸入此函數後，該函數會將陣列中的數字分別進行平方 \n",
    "  return np.square(x)\n",
    "x = np.arange(64) #生成內有數字序列的陣列\n",
    "print(x) #印出陣列中的數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qU6IzkK5M1Y",
    "outputId": "d0917ac1-2b8e-4ba8-eff1-d5b9af8a914c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.cpu_count() #輸出CPU數量，結果因電腦而異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8md259Bm5c6C",
    "outputId": "db2a03eb-3445-4941-fc37-efc18607b725"
   },
   "outputs": [],
   "source": [
    "pool = mp.Pool(8) #建立內含8個程序的多程序池（processor pool)\n",
    "squared = pool.map(square, [x[8*i:8*i+8] for i in range(8)]) #使用多程序池的.map()，對陣列中的每個數字呼叫square()，並將結果存入串列中傳回\n",
    "squared #印出結果串列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTHuqrupAd6v"
   },
   "source": [
    "# 程式 5.2: 用手動方式開啟多程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ay158cmeAg5D",
    "outputId": "bce63cf9-9a01-4911-9462-bf3c6f6975cf"
   },
   "outputs": [],
   "source": [
    "def square(i, x, queue):\n",
    "  print(\"In process {}\".format(i,))\n",
    "  queue.put(np.square(x)) #將輸出結果存進queue\n",
    "\n",
    "processes = [] #建立用來儲存不同程序的串列\n",
    "queue = mp.Queue() #建立多程序處理的Queue；該資料結構可以被不同的程序共享\n",
    "x = np.arange(64) #生成一個數列做為目標數列（內含0～63的整數）\n",
    "for i in range(8): #開啟8條程序，並讓它們利用square函式分別處理目標數列中的一部份資料\n",
    "  start_index = 8*i\n",
    "  proc = mp.Process(target=square,args=(i,x[start_index:start_index+8], queue)) \n",
    "  proc.start()\n",
    "  processes.append(proc)\n",
    "    \n",
    "for proc in processes: #{待所有程序皆執行完畢後，再將結果傳回主執行緒\n",
    "  proc.join()\n",
    "    \n",
    "for proc in processes: #終止各程序\n",
    "  proc.terminate()\n",
    "\n",
    "results = []\n",
    "while not queue.empty(): #將queue內的資料存進results串列，至到資料已清空\n",
    "  results.append(queue.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIRBb98-1ogW",
    "outputId": "b6683cd9-7688-4bfb-c407-cd98f008f43e"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDghhaXFJ3sV"
   },
   "source": [
    "# 程式 5.3：優勢演員 - 評論家的虛擬碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sI2jiwu3PmHD"
   },
   "outputs": [],
   "source": [
    "# gamma = 0.9\n",
    "# for i in epochs: #重複訓練 epochs 次\n",
    "#  state = environment.get_state() #取得環境目前的狀態\n",
    "#  value = critic(state) #價值網路預測目前狀態的價值\n",
    "#  policy = actor(state) #策略網路預測目前狀態下各動作的機率分佈\n",
    "#  action = policy.sample() #根據策略網路輸出的機率分佈選擇動作\n",
    "#  next_state, reward = environment.take_action(action) #執行動作，產生新狀態及回饋值\n",
    "#  value_next = critic(next_state) #預測新狀態的價值\n",
    "#  advantage = (reward + gamma * value_next) - value #優勢值函數\n",
    "#  loss =-1 * policy.logprob(action) * advantage #根據動作的優勢值來強化（或弱化）該動作\n",
    "#  minimize(loss) #想辦法最小化損失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqj7TMqKAfpR"
   },
   "source": [
    "# 程式 5.4： Cartpole演員-評論家模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUordcsx165-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import gym\n",
    "import torch.multiprocessing as mp \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ActorCritic(nn.Module): #定義演員—評論家模型\n",
    "  def __init__(self):\n",
    "    super(ActorCritic, self).__init__()\n",
    "    self.l1 = nn.Linear(4,25) #定義模型中各神經層的shape，參考圖5.10\n",
    "    self.l2 = nn.Linear(25,50)\n",
    "    self.actor_lin1 = nn.Linear(50,2)\n",
    "    self.l3 = nn.Linear(50,25)\n",
    "    self.critic_lin1 = nn.Linear(25,1)\n",
    "  def forward(self,x):\n",
    "    x = F.normalize(x,dim=0) #正規化輸入資料\n",
    "    y = F.relu(self.l1(x))\n",
    "    y = F.relu(self.l2(y))\n",
    "    actor = F.log_softmax(self.actor_lin1(y),dim=0) #演員端輸出遊戲中兩種可能動作的對數化機率值\n",
    "    c = F.relu(self.l3(y.detach())) #先將評論家段的節點分離，再經過ReLU的處理\n",
    "    critic = torch.tanh(self.critic_lin1(c)) #評論家端輸出一個範圍在–1到+1之間的純量\n",
    "    return actor, critic #使用tuple傳回演員和評論家的輸出結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaK5QIgoV9M5"
   },
   "source": [
    "## 設置gym環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4daf9Jo3-HG",
    "outputId": "696f6ad8-a40c-4794-d822-8d97b7f08460"
   },
   "outputs": [],
   "source": [
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0NfgyY2WBcm"
   },
   "source": [
    "## 由於要先運行程式5.6至程式5.8，因此將程式5.5移至後面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhOd_btcWTzK"
   },
   "source": [
    "# 程式 5.6： 主要訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W21sd_CG2FOB"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def worker(t, worker_model, counter, params):\n",
    "  worker_env = gym.make(\"CartPole-v1\")\n",
    "  worker_env.reset()\n",
    "  worker_opt = optim.Adam(lr=1e-4,params=worker_model.parameters()) #每條程序有獨立的運行環境和優化器，但共享模型參數\n",
    "  worker_opt.zero_grad()\n",
    "  for i in range(params['epochs']):\n",
    "    worker_opt.zero_grad()\n",
    "    values, logprobs, rewards, length = run_episode(worker_env,worker_model) #呼叫run_episode( )來執行一場遊戲並收集資料（譯註：該函式的定義見程式5.7）\n",
    "    actor_loss,critic_loss,eplen = update_params(worker_opt,values,logprobs,rewards) #使用所收集的資料來更新神經網路參數（譯註：update_params( )的定義見程式5.8）\n",
    "    counter.value = counter.value + 1 # counter是一個全域計數器，被所有程序共享\n",
    "    if(i%10 == 0): #印出當前的訓練進度\n",
    "      print(i)\n",
    "      print(len(rewards))\n",
    "      clear_output(wait=True)\n",
    "    buffer.put(length) #將遊戲長度存進buffer中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mIdjBQ2Waf0"
   },
   "source": [
    "# 程式 5.7： 執行一場遊戲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vw2-orqF95lw"
   },
   "outputs": [],
   "source": [
    "def run_episode(worker_env, worker_model):\n",
    "  state = torch.from_numpy(worker_env.env.state).float() #將環境狀態的資料型態從NumPy陣列轉換為PyTorch張量\n",
    "  values, logprobs, rewards = [],[],[] #建立三個串列，分別用來儲存狀態價值（評論家）、對數化機率分佈（演員）、以及回饋值\n",
    "  done = False\n",
    "  j=0\n",
    "  while (done == False): #除非滿足結束條件，否則遊戲繼續進行\n",
    "    j+=1\n",
    "    policy, value = worker_model(state) #計算狀態價值以及各種可能動作的對數化機率\n",
    "    values.append(value)\n",
    "    logits = policy.view(-1) #呼叫.view(-1) 將對數化機率轉成向量形式\n",
    "    action_dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = action_dist.sample() #參考演員所提供的對數化機率來選擇動作\n",
    "    logprob_ = policy.view(-1)[action]\n",
    "    logprobs.append(logprob_)\n",
    "    state_, _, done, info = worker_env.step(action.detach().numpy())\n",
    "    state = torch.from_numpy(state_).float()\n",
    "    if done: #{3}若某動作造成遊戲結束，則將回饋值設為–10，並且重置環境\n",
    "      reward = -10\n",
    "      worker_env.reset()\n",
    "    else:\n",
    "      reward = 1.0\n",
    "    rewards.append(reward)\n",
    "  return values, logprobs, rewards, len(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7bu6M-3Wc2F"
   },
   "source": [
    "# 程式 5.8： 計算並最小化損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sapSMDXfCA0l"
   },
   "outputs": [],
   "source": [
    "def update_params(worker_opt,values,logprobs,rewards,clc=0.1,gamma=0.95):\n",
    "  rewards = torch.Tensor(rewards).flip(dims=(0,)).view(-1) #{3}將rewards、logprobs及values陣列中的元素順序顛倒\n",
    "  logprobs = torch.stack(logprobs).flip(dims=(0,)).view(-1)\n",
    "  values = torch.stack(values).flip(dims=(0,)).view(-1)\n",
    "  Returns = []\n",
    "  ret_ = torch.Tensor([0])\n",
    "  for r in range(rewards.shape[0]): #使用順序巔倒後的回饋值來計算每一步的回報，並將結果存入Returns陣列中\n",
    "    ret_ = rewards[r] + gamma * ret_\n",
    "    Returns.append(ret_)\n",
    "  Returns = torch.stack(Returns).view(-1)\n",
    "  Returns = F.normalize(Returns,dim=0) #將Returns陣列中的值做正規化處理\n",
    "  actor_loss = -1*logprobs * (Returns - values.detach()) #將values張量的節點從運算圖中分離，並計算演員的損失，以避免其反向傳播到評論家端\n",
    "  critic_loss = torch.pow(values - Returns,2) #計算評論家的損失\n",
    "  loss = actor_loss.sum() + clc*critic_loss.sum() #將演員和評論家的損失加起來，變成總損失。注意，我們使用clc參數來降低評論家損失的影響\n",
    "  loss.backward()\n",
    "  worker_opt.step()\n",
    "  return actor_loss, critic_loss, len(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_7W0cDDujjO"
   },
   "source": [
    "# 程式 5.5： 分散式訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "Hf71pHC21i-i",
    "outputId": "38d9a577-3f0e-45ce-c1d6-62074fb73b28"
   },
   "outputs": [],
   "source": [
    "MasterNode = ActorCritic() #建立一個共享的全域演員—評論家實例\n",
    "MasterNode.share_memory() # share_memory()允許不同程序共用同一組模型參數（無須複製參數，節省空間）\n",
    "processes = [] #用來儲存不同程序實例的串列\n",
    "params = {\n",
    "    'epochs':500, #進行500次訓練\n",
    "    'n_workers':7, #設定程序數目為7\n",
    "}\n",
    "counter = mp.Value('i',0) #使用multiprocessing函式庫創建一個全域計數器，參數『i』代表其資料型態為整數\n",
    "buffer = mp.Queue() #編註：小編在這裡創建了一個buffer，用來儲存每一場的遊戲長度（多少回合）\n",
    "for i in range(params['n_workers']):\n",
    "  p = mp.Process(target=worker, args=(i,MasterNode,counter,params)) #啟動新的程序來運行worker函式（譯註：該函式的定義見程式5.6）\n",
    "  p.start() \n",
    "  processes.append(p)\n",
    "for p in processes: #{2}利用join讓每條程序皆完成運算後，再將結果傳回\n",
    "  p.join()\n",
    "for p in processes: #{2}終止各程序\n",
    "  p.terminate()\n",
    "\n",
    "#小編補充：原文沒有畫出平均遊戲長度的程式，因此小編加上了以下程式：\n",
    "n = params['n_workers']\n",
    "score = []\n",
    "running_mean = []\n",
    "total = torch.Tensor([0])\n",
    "mean = torch.Tensor([0])\n",
    "while not buffer.empty():\n",
    "  score.append(buffer.get()) #將buffer中的資料存入score\n",
    "print(len(score))\n",
    "for i in range (params['epochs']):\n",
    "  if (i>=50): #若訓練次數已超過50，則計算過去50場遊戲的平均長度\n",
    "    total = total - sum(score[n*(i-50) : n*(i-50)+n])/n\n",
    "    total = total + sum(score[n*i : n*i+n])/n \n",
    "    mean = int(total/50) \n",
    "  else: #若訓練次數未超過50次，則計算到目前為止的平均遊戲長度\n",
    "    total = total + sum(score[n*i : n*i+n])/n\n",
    "    mean = int(total/(i+1))\n",
    "  running_mean.append(mean)\n",
    "plt.figure(figsize=(17,12))\n",
    "plt.ylabel(\"Mean Episode Length\",fontsize=17)\n",
    "plt.xlabel(\"Training Epochs\",fontsize=17)\n",
    "plt.plot(running_mean)\n",
    "#\n",
    "print(counter.value, processes[0].exitcode) #列印全域計數器的值、以及第一個程序的退出碼（exit code，此值應為0）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiU_dqHYSI_-"
   },
   "source": [
    "# 程式 5.9： 使用N步學習來訓練Cartpole代理人\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_0Yp-I7SFmW"
   },
   "outputs": [],
   "source": [
    "def run_episode(worker_env, worker_model, N_steps=100):\n",
    "  raw_state = np.array(worker_env.env.state)\n",
    "  state = torch.from_numpy(raw_state).float()\n",
    "  values, logprobs, rewards = [],[],[]\n",
    "  done = False\n",
    "  j=0\n",
    "  check = 1\n",
    "  G=torch.Tensor([0]) #變數G代表回報，它的初始值為0\n",
    "  while (j < N_steps and done == False): #持續進行遊戲，直到經過了N步、或者遊戲結束\n",
    "    j+=1\n",
    "    policy, value = worker_model(state)\n",
    "    values.append(value)\n",
    "    logits = policy.view(-1)\n",
    "    action_dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = action_dist.sample()\n",
    "    logprob_ = policy.view(-1)[action]\n",
    "    logprobs.append(logprob_)\n",
    "    state_, _, done, info = worker_env.step(action.detach().numpy())\n",
    "    state = torch.from_numpy(state_).float()\n",
    "    if done:\n",
    "      reward = -10\n",
    "      worker_env.reset()\n",
    "      check = 1\n",
    "    else: #若遊戲並未結束，令回報值等於最新的狀態價值\n",
    "      reward = 1.0\n",
    "      G = value.detach()\n",
    "      check = 0\n",
    "    rewards.append(reward)\n",
    "  return values, logprobs, rewards, G, check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8mII1NeXeig"
   },
   "source": [
    "## Bonus - worker函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EJVxNyFXjRO"
   },
   "outputs": [],
   "source": [
    "def worker(t, worker_model, counter, params, buffer):\n",
    "  worker_env = gym.make(\"CartPole-v1\")\n",
    "  worker_env.reset()\n",
    "  worker_opt = optim.Adam(lr=1e-4,params=worker_model.parameters()) \n",
    "  worker_opt.zero_grad()\n",
    "  tot_rew = torch.Tensor([0]) \n",
    "  for i in range(params['epochs']):\n",
    "    worker_opt.zero_grad()        \n",
    "    values, logprobs, rewards, G, check = run_episode(worker_env,worker_model) \n",
    "    actor_loss,critic_loss,tot_rew = update_params(worker_opt,values,logprobs,rewards,G) \n",
    "    while(check == 0):\n",
    "      worker_opt.zero_grad()\n",
    "      values, logprobs, rewards, G, check = run_episode(worker_env,worker_model) \n",
    "      actor_loss,critic_loss,eplen = update_params(worker_opt,values,logprobs,rewards,G) \n",
    "      tot_rew += eplen\n",
    "    counter.value = counter.value + 1 \n",
    "    if(i%10 == 0):\n",
    "      print(i)\n",
    "      print(tot_rew)\n",
    "      clear_output(wait=True)\n",
    "    buffer.put(tot_rew) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk9eIza0Xm9g"
   },
   "source": [
    "## Bonus - update_params函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Uvuph6mXprc"
   },
   "outputs": [],
   "source": [
    "def update_params(worker_opt,values,logprobs,rewards,G,clc=0.1,gamma=0.95):\n",
    "  rewards = torch.Tensor(rewards).flip(dims=(0,)).view(-1) \n",
    "  logprobs = torch.stack(logprobs).flip(dims=(0,)).view(-1)\n",
    "  values = torch.stack(values).flip(dims=(0,)).view(-1)\n",
    "  Returns = []\n",
    "  ret_ = G #用G來代替原本的torch.Tensor([0])\n",
    "  for r in range(rewards.shape[0]): \n",
    "      ret_ = rewards[r] + gamma * ret_\n",
    "      Returns.append(ret_)\n",
    "  Returns = torch.stack(Returns).view(-1)\n",
    "  Returns = F.normalize(Returns,dim=0)\n",
    "  actor_loss = -1*logprobs * (Returns - values.detach()) \n",
    "  critic_loss = torch.pow(values - Returns,2) \n",
    "  loss = actor_loss.sum() + clc*critic_loss.sum() \n",
    "  loss.backward()\n",
    "  worker_opt.step()\n",
    "  return actor_loss, critic_loss, len(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcxnSvKMkiTy"
   },
   "source": [
    "##Bonus - 重建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrIiFry8kVto"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import gym\n",
    "import torch.multiprocessing as mp \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ActorCritic(nn.Module): #定義演員—評論家模型\n",
    "  def __init__(self):\n",
    "    super(ActorCritic, self).__init__()\n",
    "    self.l1 = nn.Linear(4,25)\n",
    "    self.l2 = nn.Linear(25,50)\n",
    "    self.actor_lin1 = nn.Linear(50,2)\n",
    "    self.l3 = nn.Linear(50,25)\n",
    "    self.critic_lin1 = nn.Linear(25,1)\n",
    "  def forward(self,x):\n",
    "    x = F.normalize(x,dim=0)\n",
    "    y = F.relu(self.l1(x))\n",
    "    y = F.relu(self.l2(y))\n",
    "    actor = F.log_softmax(self.actor_lin1(y),dim=0) #模型的演員端輸出遊戲中兩種可能動作的對數化機率值\n",
    "    c = F.relu(self.l3(y.detach()))\n",
    "    critic = torch.tanh(self.critic_lin1(c)) #模型的評論家端輸出一個範圍在–1到+1之間的純量\n",
    "    return actor, critic #使用tuple資料型態傳回演員和評論家的輸出結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoOI7d3pYIhH"
   },
   "source": [
    "## Bonus - 測試N-步學習法（程式5.9將N定為了100）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "id": "GTi4BDQ0YA-q",
    "outputId": "1bf964d9-2c1d-4f4e-8fcf-c8b563e71e5a"
   },
   "outputs": [],
   "source": [
    "MasterNode = ActorCritic() #A\n",
    "MasterNode.share_memory() #B\n",
    "processes = [] #C\n",
    "params = {\n",
    "    'epochs':500,\n",
    "    'n_workers':7,\n",
    "}\n",
    "buffer = mp.Queue()\n",
    "counter = mp.Value('i',0) #D\n",
    "for i in range(params['n_workers']):\n",
    "  p = mp.Process(target=worker, args=(i,MasterNode,counter,params, buffer)) #E\n",
    "  p.start() \n",
    "  processes.append(p)\n",
    "for p in processes: #F\n",
    "  p.join()\n",
    "for p in processes: #G\n",
    "  p.terminate()\n",
    "###小編補充：\n",
    "n = params['n_workers']\n",
    "score = []\n",
    "running_mean = []\n",
    "total = torch.Tensor([0])\n",
    "mean = torch.Tensor([0])\n",
    "while not buffer.empty():\n",
    "    score.append(buffer.get()) #將buffer中的資料存入score\n",
    "print(len(score))\n",
    "for i in range (params['epochs']):\n",
    "  #print(\"Epochs \", i, \": \", sum(score[n*i : n*i+n])/n)\n",
    "  if (i>=50): #{4}若訓練次數已超過50，則計算過去50場遊戲的平均長度\n",
    "    total = total - sum(score[n*(i-50) : n*(i-50)+n])/n\n",
    "    total = total + sum(score[n*i : n*i+n])/n \n",
    "    mean = int(total/50) \n",
    "  else: #{3}若訓練次數未超過50次，則計算到目前為止的遊戲長度\n",
    "    total = total + sum(score[n*i : n*i+n])/n\n",
    "    mean = int(total/(i+1))\n",
    "  #mean = sum(score[n*i : n*i+n])/n\n",
    "  running_mean.append(mean)\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.ylabel(\"Mean Episode Length\",fontsize=17)\n",
    "plt.xlabel(\"Training Epochs\",fontsize=17)\n",
    "plt.plot(running_mean)\n",
    "#\n",
    "print(counter.value,processes[0].exitcode) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiMfSZXLnDYQ"
   },
   "source": [
    "## 將N-步學習法的主函式及會用到的函式放在一起，方便執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ6rXxDePOAP"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch import optim\n",
    "# import numpy as np\n",
    "# from torch.nn import functional as F\n",
    "# import gym\n",
    "# import torch.multiprocessing as mp \n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class ActorCritic(nn.Module): #定義演員—評論家模型\n",
    "#     def __init__(self):\n",
    "#         super(ActorCritic, self).__init__()\n",
    "#         self.l1 = nn.Linear(4,25)\n",
    "#         self.l2 = nn.Linear(25,50)\n",
    "#         self.actor_lin1 = nn.Linear(50,2)\n",
    "#         self.l3 = nn.Linear(50,25)\n",
    "#         self.critic_lin1 = nn.Linear(25,1)\n",
    "#     def forward(self,x):\n",
    "#         x = F.normalize(x,dim=0)\n",
    "#         y = F.relu(self.l1(x))\n",
    "#         y = F.relu(self.l2(y))\n",
    "#         actor = F.log_softmax(self.actor_lin1(y),dim=0) #模型的演員端輸出遊戲中兩種可能動作的對數化機率值\n",
    "#         c = F.relu(self.l3(y.detach()))\n",
    "#         critic = torch.tanh(self.critic_lin1(c)) #模型的評論家端輸出一個範圍在–1到+1之間的純量\n",
    "#         return actor, critic #使用tuple資料型態傳回演員和評論家的輸出結果\n",
    "\n",
    "# def run_episode(worker_env, worker_model, N_steps=100):\n",
    "#     raw_state = np.array(worker_env.env.state)\n",
    "#     state = torch.from_numpy(raw_state).float()\n",
    "#     values, logprobs, rewards = [],[],[]\n",
    "#     done = False\n",
    "#     j=0\n",
    "#     check = 1\n",
    "#     G=torch.Tensor([0]) #變數G代表回報，它的初始值為0\n",
    "#     while (j < N_steps and done == False): #持續進行遊戲，直到執行了N個動作、或者遊戲結束\n",
    "#         j+=1\n",
    "#         policy, value = worker_model(state)\n",
    "#         values.append(value)\n",
    "#         logits = policy.view(-1)\n",
    "#         action_dist = torch.distributions.Categorical(logits=logits)\n",
    "#         action = action_dist.sample()\n",
    "#         logprob_ = policy.view(-1)[action]\n",
    "#         logprobs.append(logprob_)\n",
    "#         state_, _, done, info = worker_env.step(action.detach().numpy())\n",
    "#         state = torch.from_numpy(state_).float()\n",
    "#         if done:\n",
    "#             reward = -10\n",
    "#             worker_env.reset()\n",
    "#             check = 1\n",
    "#         else: #{3}若遊戲並未結束，令回報等於最新的狀態價值\n",
    "#             reward = 1.0\n",
    "#             G = value.detach()\n",
    "#             check = 0\n",
    "#         rewards.append(reward)\n",
    "#     return values, logprobs, rewards, G, check\n",
    "\n",
    "# def update_params(worker_opt,values,logprobs,rewards,G,clc=0.1,gamma=0.95):\n",
    "#         rewards = torch.Tensor(rewards).flip(dims=(0,)).view(-1) #A\n",
    "#         logprobs = torch.stack(logprobs).flip(dims=(0,)).view(-1)\n",
    "#         values = torch.stack(values).flip(dims=(0,)).view(-1)\n",
    "#         Returns = []\n",
    "#         ret_ = G\n",
    "#         for r in range(rewards.shape[0]): #B\n",
    "#             ret_ = rewards[r] + gamma * ret_\n",
    "#             Returns.append(ret_)\n",
    "#         Returns = torch.stack(Returns).view(-1)\n",
    "#         Returns = F.normalize(Returns,dim=0)\n",
    "#         actor_loss = -1*logprobs * (Returns - values.detach()) #C\n",
    "#         critic_loss = torch.pow(values - Returns,2) #D\n",
    "#         loss = actor_loss.sum() + clc*critic_loss.sum() #E\n",
    "#         loss.backward()\n",
    "#         worker_opt.step()\n",
    "#         return actor_loss, critic_loss, len(rewards)\n",
    "\n",
    "# def worker(t, worker_model, counter, params, buffer):\n",
    "#     worker_env = gym.make(\"CartPole-v1\")\n",
    "#     worker_env.reset()\n",
    "#     worker_opt = optim.Adam(lr=1e-4,params=worker_model.parameters()) #A\n",
    "#     worker_opt.zero_grad()\n",
    "#     tot_rew = torch.Tensor([0]) \n",
    "#     for i in range(params['epochs']):\n",
    "#         worker_opt.zero_grad()        \n",
    "#         values, logprobs, rewards, G, check = run_episode(worker_env,worker_model) #B \n",
    "#         actor_loss,critic_loss,tot_rew = update_params(worker_opt,values,logprobs,rewards,G) #C\n",
    "#         while(check == 0):\n",
    "#           worker_opt.zero_grad()\n",
    "#           values, logprobs, rewards, G, check = run_episode(worker_env,worker_model) #B \n",
    "#           actor_loss,critic_loss,eplen = update_params(worker_opt,values,logprobs,rewards,G) #C\n",
    "#           tot_rew += eplen\n",
    "#         counter.value = counter.value + 1 #D\n",
    "#         #print(\"Training \", i,\": \",len(rewards))\n",
    "#         if(i%10 == 0):\n",
    "#           print(i)\n",
    "#           print(tot_rew)\n",
    "#           clear_output(wait=True)\n",
    "#         buffer.put(tot_rew) \n",
    "\n",
    "# MasterNode = ActorCritic() #A\n",
    "# MasterNode.share_memory() #B\n",
    "# processes = [] #C\n",
    "# params = {\n",
    "#     'epochs':500,\n",
    "#     'n_workers':7,\n",
    "# }\n",
    "# buffer = mp.Queue()\n",
    "# counter = mp.Value('i',0) #D\n",
    "# for i in range(params['n_workers']):\n",
    "#     p = mp.Process(target=worker, args=(i,MasterNode,counter,params, buffer)) #E\n",
    "#     p.start() \n",
    "#     processes.append(p)\n",
    "# for p in processes: #F\n",
    "#     p.join()\n",
    "# for p in processes: #G\n",
    "#     p.terminate()\n",
    "# ###小編補充：\n",
    "# n = params['n_workers']\n",
    "# score = []\n",
    "# running_mean = []\n",
    "# total = torch.Tensor([0])\n",
    "# mean = torch.Tensor([0])\n",
    "# while not buffer.empty():\n",
    "#     score.append(buffer.get()) #將buffer中的資料存入score\n",
    "# print(len(score))\n",
    "# for i in range (params['epochs']):\n",
    "#   #print(\"Epochs \", i, \": \", sum(score[n*i : n*i+n])/n)\n",
    "#   if (i>=50): #{4}若訓練次數已超過50，則計算過去50場遊戲的平均長度\n",
    "#     total = total - sum(score[n*(i-50) : n*(i-50)+n])/n\n",
    "#     total = total + sum(score[n*i : n*i+n])/n \n",
    "#     mean = int(total/50) \n",
    "#   else: #{3}若訓練次數未超過50次，則計算到目前為止的遊戲長度\n",
    "#     total = total + sum(score[n*i : n*i+n])/n\n",
    "#     mean = int(total/(i+1))\n",
    "#   #mean = sum(score[n*i : n*i+n])/n\n",
    "#   running_mean.append(mean)\n",
    "# plt.figure(figsize=(20,12))\n",
    "# plt.ylabel(\"Mean Episode Length\",fontsize=15)\n",
    "# plt.xlabel(\"Training Epochs\",fontsize=15)\n",
    "# plt.plot(running_mean)\n",
    "# #\n",
    "# print(counter.value,processes[0].exitcode) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmCaoLFrvz4z"
   },
   "source": [
    "# 程式 5.10: bootstrap對回報值的影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbR2XV0Nv3Ht",
    "outputId": "72749bca-de92-4495-9c4b-1a10f312ac15"
   },
   "outputs": [],
   "source": [
    "#兩場遊戲（長度為3步）的模擬回饋值\n",
    "r1 = [1,1,-1]\n",
    "r2 = [1,1,1]\n",
    "R1,R2 = 0.0,0.0 #初始化兩場遊戲的回報為0\n",
    "\n",
    "#不使用bootstrap\n",
    "for i in range(len(r1)-1,0,-1): #i的值從len(r1)-1開始，一直往後遞減至0（不包含0）\n",
    "  R1 = r1[i] + 0.99*R1\n",
    "for i in range(len(r2)-1,0,-1):\n",
    "  R2 = r2[i] + 0.99*R2\n",
    "print(\"No bootstrapping\")\n",
    "print(R1,R2)\n",
    "\n",
    "#使用bootstrap\n",
    "R1,R2 = 1.0,1.0 #利用bootstrap事先預測遊戲的回報值皆為1\n",
    "for i in range(len(r1)-1,0,-1):\n",
    "  R1 = r1[i] + 0.99*R1\n",
    "for i in range(len(r2)-1,0,-1):\n",
    "  R2 = r2[i] + 0.99*R2\n",
    "print(\"With bootstrapping\")\n",
    "print(R1,R2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "F1384_第5章.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
